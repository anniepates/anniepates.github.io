---
layout: post
title: Week 3
---

This week, I started working on writing the algorithm to perform keyword extraction that will optimize the search for 3-D object descriptions. After my research last week, I knew there were several resources in the spaCy library that would help me with this, so I began playing around with its tools. Before focusing on the keywords, I needed to perform some basic NLP operations like preprocessing (cleaning and normalizing), tokenization (splitting the text into individual words), stemming/lemmatization (reducing words to their root form). Luckily, spaCy has incredible built-in packaging that I was able to take advantage of for this task. The final bit of preprocessing I did was to write a function that extracts nouns and adjectives from text input since these are most likely to be the important parts of a query looking for objects.

To simplify my initial approach, I am using a list of the top 15 keywords that appear in the complete set of 3D models of assistive designs as identified by researchers including my advisor Prof. Hofmann in the 2015 paper <a href="https://dl.acm.org/doi/10.1145/2702123.2702525" target="_blank" rel="noopener noreferrer">Sharing is Caring: Assistive Technology Designs on Thingiverse</a>. For a basic starter dataset, I pulled a few a few descriptions of objects that came up when I searched for those terms on Thingiverse. Next, I look for those keywords in a sample query and examine how many of the keywords in the query can be found in the sample descriptions. I return any matching descriptions sorted by relevance. For a relevance metric, I chose to use term frequency-inverse document frequency which balances how often a term appears in a document with how many documents it appears in.

Right now, my approach is successfully identifying relevant documents in the sample set, but it is very limited in scope by these 15 keywords. It may in fact be overfitting to those keywords by not returning similar objects that do not match the exact term, so I would like to work on something more generalizable.
