---
layout: post
title: Week 2
---

This past week, I dedicated myself to understanding different Natural Language Processing techniques and determining which ones will be most helpful for my work in optimizing search query results. This was an interesting undertaking for me since I do not have any experience with NLP yet (though I am signed up for a course in the fall!). I determined that while I will need to use some techniques like part-of-speech analysis and lemmatization, the concepts that were most important for me to understand were keyword extraction and summarization. I read deeper on how techniques like convolutional neural networks can be used to extract custom keywords or recognize universal named entities in text.

Through this survey, I learned about several libraries and tools that perform NLP tasks. A 2019 paper by <a href="https://ieeexplore.ieee.org/document/8931850" target="_blank" rel="noopener noreferrer">Schmitt et al</a> helped me understand the potential strengths and weaknesses between different models including Stanford NLP, NTLK, and spaCy. Special thanks to fellow DREAMer Max Tseng for lending his expertise on different NLP software. Based on this research and a need for Python compatibility, I have decided to primarily use spaCy which is especially good for keyword extraction and named entity recognition.

All this research led to a first draft of the Related Works section of the paper I hope to write over the couse of my project. It also helped me understand the scope of what is possible for my contribution and where I can begin. Now to start putting it into practice!

